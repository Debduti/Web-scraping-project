{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping Indeed to list down all openings got Data Analyst Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries as required. Beautiful Soup will help in parsing the html code in the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We go to the Indeed Canada website and run a generic search for Data Analyst openings. We grab the URL and use that in our code to parse the requisite html tags.\n",
    "### We create a list of a few cities we want to check out job openings for. We pick up the information present for Job Title,Company Name,Job Summary,and Date of posting, and arrange them in contiguous columns. We can add or remove as many columns as we want to. We loop through all the openings for all the cities in the list and save them in a CSV file.\n",
    "### Most of the required information can be found under the tags div and  span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'sample.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e1c47d921469>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0msample_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob_post\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m#Saving scrape results in a CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0msample_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'sample.csv'"
     ]
    }
   ],
   "source": [
    "max_results_per_city = 100\n",
    "\n",
    "city_set = ['Toronto','Missisauga','Markham']\n",
    "\n",
    "columns = ['City', 'Job_title', 'Company_name', 'Summary','Date_Posted']\n",
    "sample_df = pd.DataFrame(columns = columns)\n",
    "\n",
    "for city in city_set:\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        page = requests.get('https://ca.indeed.com/jobs?q=data+analyst&l=' + str(city) + '&start=' + str(start))\n",
    "        time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "        soup = BeautifulSoup(page.text, 'lxml', from_encoding='utf-8')\n",
    "        for div in soup.find_all(name='div', attrs={'class':'row'}): \n",
    "            #specifying row num for index of job posting in dataframe\n",
    "            num = (len(sample_df) + 1) \n",
    "            #creating an empty list to hold the data for each posting\n",
    "            job_post = [] \n",
    "            #append city name--> 1\n",
    "            job_post.append(city)        \n",
    "            \n",
    "            # Extract Job Title-->2\n",
    "            for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "                job_post.append(a['title'])\n",
    "                \n",
    "            # Extract Company Name-->3\n",
    "            company = div.find_all(name='span', attrs={'class':'company'})\n",
    "            if len(company) > 0:\n",
    "                for b in company:\n",
    "                    job_post.append(b.text.strip())\n",
    "\n",
    "            else:\n",
    "                sec_try = div.find_all(name='span', attrs={'class':'result-link-source'})\n",
    "                for span in sec_try:\n",
    "                    job_post.append(span.text.strip())\n",
    "                    \n",
    "            #Extract Summary-->5\n",
    "            spans = div.findAll('div', attrs={'class': 'summary'})\n",
    "            for span in spans:\n",
    "                job_post.append(span.text.strip())\n",
    "                \n",
    "            #Extract Date  of Posting-->6\n",
    "            posting = div.findAll('span', attrs={'class': 'date'})\n",
    "            for post in posting:\n",
    "                job_post.append(post.text.strip())\n",
    "            #Appending list of job post info to dataframe at index num\n",
    "            sample_df.loc[num] = job_post\n",
    "#Saving scrape results in a CSV file\n",
    "sample_df.to_csv('sample.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Project Administrator/Data Analyst</td>\n",
       "      <td>Demant</td>\n",
       "      <td>Integrate data from multiple sources (CRM, Nav...</td>\n",
       "      <td>30+ days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Analytics &amp; Insights Analyst</td>\n",
       "      <td>AIR MILES</td>\n",
       "      <td>Business Skills - Expert knowledge in campaign...</td>\n",
       "      <td>Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Data Management Analyst - 294161</td>\n",
       "      <td>Procom</td>\n",
       "      <td>Support data quality exception reporting and d...</td>\n",
       "      <td>11 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Sr.Analyst, Insights</td>\n",
       "      <td>AIR MILES</td>\n",
       "      <td>This role bridges client’s business problems w...</td>\n",
       "      <td>Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Finance and Data Analyst</td>\n",
       "      <td>UPS</td>\n",
       "      <td>Data analytical : 1 year (Required).\\nProduces...</td>\n",
       "      <td>20 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Data Analyst (Remote)</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>Strongly desired: understanding of economic da...</td>\n",
       "      <td>Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Achievers</td>\n",
       "      <td>Develop and improve data processes and best pr...</td>\n",
       "      <td>7 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>DATA ANALYST - DATA ENGINEERING</td>\n",
       "      <td>Allstate Canada</td>\n",
       "      <td>Acquire data from primary or secondary data so...</td>\n",
       "      <td>30+ days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Data Analyst - 4 month Co-op</td>\n",
       "      <td>Flashfood</td>\n",
       "      <td>Knowledge of software engineering, data engine...</td>\n",
       "      <td>7 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Brookfield Asset Management</td>\n",
       "      <td>Analyze large data sets with multiple data sou...</td>\n",
       "      <td>10 days ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city                           job_title                 company_name  \\\n",
       "1   Toronto  Project Administrator/Data Analyst                       Demant   \n",
       "2   Toronto        Analytics & Insights Analyst                    AIR MILES   \n",
       "3   Toronto    Data Management Analyst - 294161                       Procom   \n",
       "4   Toronto                Sr.Analyst, Insights                    AIR MILES   \n",
       "5   Toronto            Finance and Data Analyst                          UPS   \n",
       "6   Toronto               Data Analyst (Remote)                         Yelp   \n",
       "7   Toronto                        Data Analyst                    Achievers   \n",
       "8   Toronto     DATA ANALYST - DATA ENGINEERING              Allstate Canada   \n",
       "9   Toronto        Data Analyst - 4 month Co-op                    Flashfood   \n",
       "10  Toronto                        Data Analyst  Brookfield Asset Management   \n",
       "\n",
       "                                              summary          date  \n",
       "1   Integrate data from multiple sources (CRM, Nav...  30+ days ago  \n",
       "2   Business Skills - Expert knowledge in campaign...         Today  \n",
       "3   Support data quality exception reporting and d...   11 days ago  \n",
       "4   This role bridges client’s business problems w...         Today  \n",
       "5   Data analytical : 1 year (Required).\\nProduces...   20 days ago  \n",
       "6   Strongly desired: understanding of economic da...         Today  \n",
       "7   Develop and improve data processes and best pr...    7 days ago  \n",
       "8   Acquire data from primary or secondary data so...  30+ days ago  \n",
       "9   Knowledge of software engineering, data engine...    7 days ago  \n",
       "10  Analyze large data sets with multiple data sou...   10 days ago  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_df.head(10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
